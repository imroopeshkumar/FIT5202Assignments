{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SparkSession is created using a SparkConf object, which would use two local cores\n",
    "with a proper application name, and use UTC as the timezone 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[2]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Linux system hacking Detection\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "#Method 1: Using SparkSession\n",
    "#We have to keep the maxPartition bytes by seeing the size of our csv files and in such a way we get 4 partitions\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"UTC\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From the Kafka producers in Task 1.1 and 1.2, ingest the streaming data into Spark\n",
    "Streaming for both process and memory activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"process\"\n",
    "df_process = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"memory\"\n",
    "df_memory = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process = df_process.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = df_memory.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the schemas for both the activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_process = ArrayType(StructType([    \n",
    "    StructField('sequence', IntegerType(), True), \n",
    "    StructField('machine', IntegerType(), True),\n",
    "    StructField('PID', IntegerType(), True),\n",
    "    StructField('TRUN', IntegerType(), True),\n",
    "    StructField('TSLPI', IntegerType(), True),\n",
    "    StructField('TSLPU', IntegerType(), True),\n",
    "    StructField('POLI', StringType(), True),\n",
    "    StructField('NICE', IntegerType(), True),\n",
    "    StructField('PRI', IntegerType(), True),\n",
    "    StructField('RTPR', IntegerType(), True),\n",
    "    StructField('CPUNR', IntegerType(), True),\n",
    "    StructField('Status', StringType(), True),\n",
    "    StructField('EXC', IntegerType(), True),\n",
    "    StructField('State', StringType(), True),\n",
    "    StructField('CPU', FloatType(), True),\n",
    "    StructField('CMD', StringType(), True),\n",
    "    StructField('ts', StringType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process=df_process.select(F.from_json(F.col(\"value\").cast(\"string\"), schema_process).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process = df_process.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_formatted = df_process.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "                    F.col(\"unnested_value.TRUN\").alias(\"TRUN\"),\n",
    "                    F.col(\"unnested_value.TSLPI\").alias(\"TSLPI\"),\n",
    "                    F.col(\"unnested_value.TSLPU\").alias(\"TSLPU\"),\n",
    "                    F.col(\"unnested_value.POLI\").alias(\"POLI\"),\n",
    "                    F.col(\"unnested_value.NICE\").alias(\"NICE\"),\n",
    "                    F.col(\"unnested_value.PRI\").alias(\"PRI\"),\n",
    "                    F.col(\"unnested_value.RTPR\").alias(\"RTPR\"),\n",
    "                    F.col(\"unnested_value.CPUNR\").alias(\"CPUNR\"),\n",
    "                    F.col(\"unnested_value.Status\").alias(\"Status\"),\n",
    "                    F.col(\"unnested_value.EXC\").alias(\"EXC\"),\n",
    "                    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "                    F.col(\"unnested_value.CPU\").alias(\"CPU\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the same for the memory activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_memory = ArrayType(StructType([    \n",
    "    StructField('sequence', IntegerType(), True), \n",
    "    StructField('machine', IntegerType(), True),\n",
    "    StructField('PID', LongType(), True),\n",
    "    StructField('MINFLT', StringType(), True),\n",
    "    StructField('MAJFLT', StringType(), True),\n",
    "    StructField('VSTEXT', StringType(), True),\n",
    "    StructField('VSIZE', DoubleType(), True),\n",
    "    StructField('RSIZE', StringType(), True),\n",
    "    StructField('VGROW', StringType(), True),\n",
    "    StructField('RGROW', StringType(), True),\n",
    "    StructField('MEM', FloatType(), True),\n",
    "    StructField('CMD', StringType(), True),\n",
    "    StructField('ts', StringType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory=df_memory.select(F.from_json(F.col(\"value\").cast(\"string\"), schema_memory).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = df_memory.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_formatted = df_memory.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "                    F.col(\"unnested_value.MINFLT\").alias(\"MINFLT\"),\n",
    "                    F.col(\"unnested_value.MAJFLT\").alias(\"MAJFLT\"),\n",
    "                    F.col(\"unnested_value.VSTEXT\").alias(\"VSTEXT\"),\n",
    "                    F.col(\"unnested_value.VSIZE\").alias(\"VSIZE\"),\n",
    "                    F.col(\"unnested_value.RSIZE\").alias(\"RSIZE\"),\n",
    "                    F.col(\"unnested_value.VGROW\").alias(\"VGROW\"),\n",
    "                    F.col(\"unnested_value.RGROW\").alias(\"RGROW\"),\n",
    "                    F.col(\"unnested_value.MEM\").alias(\"MEM\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to show values received from input dataframe\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = df_memory_formatted.writeStream.outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .trigger(processingTime='5 seconds')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Then the streaming data format should be transformed into the proper formats\n",
    "following the metadata file schema for both process and memory, similar to\n",
    "assignment 2A 4 (3%)\n",
    "- The numeric values with extra spaces or “K” / “M” / “G” should be properly\n",
    "transformed into their correct values\n",
    "- The NICE value should also be restored based on the PRI values using their\n",
    "relationship 5\n",
    "- Hint - There is a mapping between PRI (priority) and NICE, as long as\n",
    "the process is not yet finished during the last interval. For example,\n",
    "- PRI 100 maps to NICE -20\n",
    "- PRI 101 maps toNICE -19\n",
    "- …\n",
    "- PRI 139 maps to NICE 19\n",
    "- Hint - If the process is finished, PRI and NICE would both be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data = udf(lambda z:float(z[0:-1])*1000 if 'K' in z\\\n",
    "           else (float(z[0:-1])*1000000 if 'M' in z\\\n",
    "                 else (float(z[0:-1])*1000000000 if 'G' in z\\\n",
    "                     else ( z.replace(\" \",\"\") if \" \" in z\\\n",
    "                           else z))))\n",
    "\n",
    "cols = ['MINFLT', 'MAJFLT', 'VSTEXT', 'RSIZE', 'VGROW', 'RGROW']\n",
    "for column in cols:\n",
    "    df_memory_formatted = df_memory_formatted.withColumn(column, correct_data(col(column)).cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data = udf(lambda z:int(z-120) if z!=0 \n",
    "                  else 0)\n",
    "\n",
    "df_process_formatted = df_process_formatted.withColumn(\"NICE\", correct_data(\"PRI\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_formatted = df_memory_formatted.withColumn('CMD_PID', F.concat(F.col('CMD'),F.lit('_'), F.col('PID')))\n",
    "\n",
    "df_memory_formatted = df_memory_formatted.withColumn(\"event_time\",F.col('ts').cast('timestamp'))\n",
    "\n",
    "memory_watermark = df_memory_formatted \\\n",
    "    .withWatermark(\"event_time\", \"20 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_process_formatted = df_process_formatted \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"process_formatted_sql\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from process_formatted_sql\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_formatted = df_process_formatted.withColumn('CMD_PID', F.concat(F.col('CMD'),F.lit('_'), F.col('PID')))\n",
    "\n",
    "df_process_formatted = df_process_formatted.withColumn(\"event_time\",F.col('ts').cast('timestamp'))\n",
    "\n",
    "process_watermark = df_process_formatted \\\n",
    "    .withWatermark(\"event_time\", \"20 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_memory_sink = df_memory_formatted.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"process.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"process.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_memory_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_process_sink = df_process_formatted.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"memory.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"./checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_process_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPath =  \"../process_pipeline_model\"\n",
    "\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "pipelineModel = PipelineModel.load(mPath)\n",
    "\n",
    "df_process_prediction = pipelineModel.transform(df_process_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPath =  \"../memory_pipeline_model\"\n",
    "\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "pipelineModel = PipelineModel.load(mPath)\n",
    "\n",
    "df_memory_prediction = pipelineModel.transform(df_memory_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_prediction = pipelineModel.transform(df_memory_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_attack_count = df_memory_prediction \\\n",
    "    .filter(df_memory_prediction['prediction'] == 1.0)\\\n",
    "    .groupBy(window(df_memory_prediction.event_time, \"120 seconds\"), df_memory_prediction['machine'].alias(\"machine_id\"), df_memory_prediction['CMD_PID'].alias(\"CMD_PID\"))\\\n",
    "    .agg(F.sum(\"prediction\").alias(\"count\"))\\\n",
    "    .select(\"machine_id\", \"window\", \"count\")\\\n",
    "    .sort(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_memory_attack_count = df_memory_attack_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"memory_attack_count_sql\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from memory_attack_count_sql\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_attack_count = df_process_prediction \\\n",
    "    .filter(df_process_prediction['prediction'] == 1.0)\\\n",
    "    .groupBy(window(df_process_prediction.event_time, \"120 seconds\"), df_process_prediction['machine'].alias(\"machine_id\"), df_process_prediction['CMD_PID'].alias(\"CMD_PID\"))\\\n",
    "    .agg(F.sum(\"prediction\").alias(\"count\"))\\\n",
    "    .select(\"machine_id\", \"window\", \"count\")\\\n",
    "    .sort(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_process_attack_count = df_process_attack_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"process_attack_count_sql\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from process_attack_count_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_attack_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_plots():\n",
    "    try:\n",
    "        width = 9.5\n",
    "        height = 6\n",
    "        fig = plt.figure(figsize=(width,height)) # create new figure\n",
    "        fig.subplots_adjust(hspace=0.8)\n",
    "        ax = fig.add_subplot(111) # adding the subplot axes to the given grid position\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.title.set_text('Time Vs Value')\n",
    "        fig.suptitle('Real-time uniform stream data visualization') # giving figure a title\n",
    "        fig.show() # displaying the figure\n",
    "        fig.canvas.draw() # drawing on the canvas\n",
    "        return fig, ax\n",
    "    except Exception as ex:\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = init_plots()\n",
    "\n",
    "while True:\n",
    "    df_all = spark.sql(\"select * from memory_attack_count_sql order by machine_id\").toPandas()\n",
    "    # Get starting timestamp to plot both graphs\n",
    "    start_time = df_all['window'][len(df_all)-1]\n",
    "    df_reduced = spark.sql(\"select * from reduced_values where end_time>='\"+str(start_time)+\"' order by end_time desc\").toPandas()\n",
    "    \n",
    "    x_all = df_all['window'].to_list()\n",
    "    y_all = df_all['value'].to_list()\n",
    "    x_reduced = df_reduced['end_time'].to_list()\n",
    "    y_reduced = df_reduced['avg_value'].to_list()\n",
    "    ax.clear()\n",
    "    ax.plot(x_all, y_all, '-b', label='Original')\n",
    "    ax.plot(x_reduced, y_reduced, '--r', label='Reduced')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    leg = ax.legend()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = init_plots()\n",
    "\n",
    "while True:\n",
    "    df = spark.sql(\"select * from process_attack_count_sql\").toPandas()\n",
    "    \n",
    "    if(len(df)>0):        \n",
    "        x = df['word'].to_list()\n",
    "        y = df['count'].to_list() \n",
    "        ax.clear()\n",
    "        ax.plot(x, y)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')\n",
    "        fig.canvas.draw()\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
